{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random, math, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "num_arms = 3\n",
    "\n",
    "# Oracle theta of each arm\n",
    "theta = np.array([0.7, 0.5, 0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Implement classical bandit algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The epsilon-greedy Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(epsilon, N, theta):\n",
    "    \"\"\"\n",
    "    Implement the epsilon-greedy algorithm for a Bernoulli bandit problem.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epsilon : float\n",
    "        The probability of exploration.\n",
    "    counts : array-like\n",
    "        Count of how many times each arm is pulled\n",
    "    N : int\n",
    "        Number of time steps.\n",
    "    theta : array-like\n",
    "        True success probabilities of each arm.\n",
    "    Q : array-like\n",
    "        Estimated values for each arm.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rewards_history : list\n",
    "        Rewards obtained at each time step.\n",
    "    \"\"\"\n",
    "    \n",
    "    Q = np.zeros(num_arms)\n",
    "    counts = np.zeros(num_arms)\n",
    "    total_reward = 0;\n",
    "    \n",
    "    for t in range(1, N+1):\n",
    "        # Choose arm using epsilon-greedy\n",
    "        if np.random.rand() < epsilon:\n",
    "            # Exploration: choose a random arm\n",
    "            arm = np.random.randint(num_arms)\n",
    "        else:\n",
    "            # Exploitation: choose the best arm so far\n",
    "            arm = np.argmax(Q)\n",
    "        \n",
    "        # Simulate pulling the chosen arm and get reward\n",
    "        reward = 1 if np.random.rand() < theta[arm] else 0\n",
    "        \n",
    "        # Update counts and estimates\n",
    "        counts[arm] += 1\n",
    "        Q[arm] = Q[arm] + (1/counts[arm])*(reward - Q[arm])\n",
    "        \n",
    "        total_reward += reward\n",
    "    \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The UCB (Upper Confidence Bound) Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucb(c, N, theta):\n",
    "    \"\"\"\n",
    "    Implement the UCB (Upper Confidence Bound) algorithm for a Bernoulli bandit problem.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    c : float\n",
    "        Confidence level parameter for the UCB algorithm.\n",
    "    N : int\n",
    "        Number of time steps.\n",
    "    theta : array-like\n",
    "        True success probabilities of each arm.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    rewards_history : array\n",
    "        The rewards obtained at each time step.\n",
    "    \"\"\"\n",
    "\n",
    "    Q = np.zeros(num_arms)\n",
    "    counts = np.zeros(num_arms)\n",
    "    total_reward = 0\n",
    "    \n",
    "    # Initialize by pulling each arm once\n",
    "    for arm in range(num_arms):\n",
    "        reward = 1 if np.random.rand() < theta[arm] else 0\n",
    "        Q[arm] = reward\n",
    "        counts[arm] = 1\n",
    "        total_reward += reward\n",
    "\n",
    "    for t in range(num_arms+1, N+1):\n",
    "        # Avoid division by zero because each arm was pulled once\n",
    "        ucb_values = Q + c * np.sqrt((2*np.log(t))/counts)\n",
    "        arm = np.argmax(ucb_values)\n",
    "        reward = 1 if np.random.rand() < theta[arm] else 0\n",
    "        counts[arm] += 1\n",
    "        Q[arm] += (1/counts[arm])*(reward - Q[arm])\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TS (Thompson Sampling) Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta\n",
    "\n",
    "def thompson_sampling(N, theta, alpha_init, beta_init):\n",
    "    \"\"\"\n",
    "    Implement the Thompson Sampling (TS) algorithm for a Bernoulli bandit problem.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    N : int\n",
    "        Number of time steps.\n",
    "    theta : array-like\n",
    "        True success probabilities of each arm.\n",
    "    alpha_init : array-like\n",
    "        Initial alpha parameters for the Beta distributions of each arm.\n",
    "    beta_init : array-like\n",
    "        Initial beta parameters for the Beta distributions of each arm.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    rewards_history : array\n",
    "        The rewards obtained at each time step.\n",
    "    \"\"\"\n",
    "    alpha = alpha_init.copy()\n",
    "    beta_ = beta_init.copy()\n",
    "    total_reward = 0\n",
    "    for t in range(N):\n",
    "        sampled_thetas = [np.random.beta(alpha[j], beta_[j]) for j in range(num_arms)]\n",
    "        arm = np.argmax(sampled_thetas)\n",
    "        reward = 1 if np.random.rand() < theta[arm] else 0\n",
    "        total_reward += reward\n",
    "        alpha[arm] += reward\n",
    "        beta_[arm] += 1 - reward\n",
    "    return total_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Each experiment lasts for $N = 5000$ time slots, and we run each experiment $200$ trials. Results are averaged over these $200$ independent trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "N = 5000\n",
    "num_trials = 200\n",
    "epsilons = [0.1, 0.5, 0.9]\n",
    "cs = [1, 5, 10]\n",
    "\n",
    "# Two sets of prior parameters for TS\n",
    "# Set 1: (1,1), (1,1), (1,1)\n",
    "alpha_set_1 = np.array([1, 1, 1])\n",
    "beta_set_1 = np.array([1, 1, 1])\n",
    "\n",
    "# Set 2: (601,401), (401,601), (2,3)\n",
    "alpha_set_2 = np.array([601, 401, 2])\n",
    "beta_set_2 = np.array([401, 601, 3])\n",
    "\n",
    "# True parameters of the arms (as per the oracle, but not known to the algorithm)\n",
    "theta = np.array([0.7, 0.5, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon-greedy results:\n",
      "Epsilon = 0.1, Average total reward over 200 trials: 3415.575\n",
      "Epsilon = 0.5, Average total reward over 200 trials: 3080.795\n",
      "Epsilon = 0.9, Average total reward over 200 trials: 2750.63\n"
     ]
    }
   ],
   "source": [
    "# Epsilon-greedy\n",
    "print(\"Epsilon-greedy results:\")\n",
    "for eps in epsilons:\n",
    "    rewards = []\n",
    "    for _ in range(num_trials):\n",
    "        rewards.append(epsilon_greedy(eps, N, theta))\n",
    "    mean_reward = np.mean(rewards)\n",
    "    print(f\"Epsilon = {eps}, Average total reward over {num_trials} trials: {mean_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UCB results:\n",
      "c = 1, Average total reward over 200 trials: 3411.855\n",
      "c = 5, Average total reward over 200 trials: 2976.415\n",
      "c = 10, Average total reward over 200 trials: 2826.26\n"
     ]
    }
   ],
   "source": [
    "# UCB\n",
    "print(\"\\nUCB results:\")\n",
    "for c_val in cs:\n",
    "    rewards = []\n",
    "    for _ in range(num_trials):\n",
    "        rewards.append(ucb(c_val, N, theta))\n",
    "    mean_reward = np.mean(rewards)\n",
    "    print(f\"c = {c_val}, Average total reward over {num_trials} trials: {mean_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thompson Sampling results:\n",
      "Set 1 Priors (1,1),(1,1),(1,1), Average total reward: 3477.98\n",
      "Set 2 Priors (601,401),(401,601),(2,3), Average total reward: 3492.285\n"
     ]
    }
   ],
   "source": [
    "# Thompson Sampling\n",
    "print(\"\\nThompson Sampling results:\")\n",
    "rewards_set_1 = []\n",
    "for _ in range(num_trials):\n",
    "    rewards_set_1.append(thompson_sampling(N, theta, alpha_set_1, beta_set_1))\n",
    "mean_set_1 = np.mean(rewards_set_1)\n",
    "print(f\"Set 1 Priors (1,1),(1,1),(1,1), Average total reward: {mean_set_1}\")\n",
    "\n",
    "rewards_set_2 = []\n",
    "for _ in range(num_trials):\n",
    "    rewards_set_2.append(thompson_sampling(N, theta, alpha_set_2, beta_set_2))\n",
    "mean_set_2 = np.mean(rewards_set_2)\n",
    "print(f\"Set 2 Priors (601,401),(401,601),(2,3), Average total reward: {mean_set_2}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PROBABILITY_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
